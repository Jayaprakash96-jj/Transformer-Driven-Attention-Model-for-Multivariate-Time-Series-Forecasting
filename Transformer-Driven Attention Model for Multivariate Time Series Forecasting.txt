import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error, mean_absolute_error

#1.GENERATE MULTIVARIATE TIME SERIES DATA (10 features, 10,000 samples)
def generate_series(n_samples=10000, n_features=10):
    t = np.arange(n_samples)
    data = []

    for i in range(n_features):
        trend = 0.01 * t
        season = np.sin(0.02 * t + i)
        noise = np.random.normal(0, 0.2, n_samples)
        feature = trend + season + noise
        data.append(feature)

    return np.array(data).T

data = generate_series()
df = pd.DataFrame(data, columns=[f"feature_{i}" for i in range(10)])

#2.DATASET CLASS
class TimeSeriesDataset(Dataset):
    def __init__(self, data, seq_len=50):
        self.data = data
        self.seq_len = seq_len

    def __len__(self):
        return len(self.data) - self.seq_len

    def __getitem__(self, idx):
        x = self.data[idx : idx + self.seq_len]
        y = self.data[idx + self.seq_len]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

#3.POSITIONAL ENCODING
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(pos * div_term)
        pe[:, 1::2] = torch.cos(pos * div_term)

        self.pe = pe.unsqueeze(0)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

#4.SCALED DOT-PRODUCT ATTENTION
class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, Q, K, V):
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)
        weights = torch.softmax(scores, dim=-1)
        return torch.matmul(weights, V)

#5.MULTI-HEAD ATTENTION
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=64, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.fc = nn.Linear(d_model, d_model)

        self.attn = ScaledDotProductAttention()

    def forward(self, x):
        batch, seq_len, d_model = x.size()

        Q = self.W_q(x).view(batch, seq_len, self.num_heads, self.d_k)
        K = self.W_k(x).view(batch, seq_len, self.num_heads, self.d_k)
        V = self.W_v(x).view(batch, seq_len, self.num_heads, self.d_k)

        Q = Q.transpose(1,2)
        K = K.transpose(1,2)
        V = V.transpose(1,2)

        ctx = self.attn(Q, K, V)

        ctx = ctx.transpose(1,2).contiguous().view(batch, seq_len, d_model)
        return self.fc(ctx)

#6.TRANSFORMER BLOCK
class TransformerBlock(nn.Module):
    def __init__(self, d_model=64, num_heads=4, expansion=128):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)

        self.ff = nn.Sequential(
            nn.Linear(d_model, expansion),
            nn.ReLU(),
            nn.Linear(expansion, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        att = self.mha(x)
        x = self.norm1(x + att)

        ff = self.ff(x)
        x = self.norm2(x + ff)

        return x

#7.class TransformerModel(nn.Module):
    def __init__(self, feature_dim=10, d_model=64, num_heads=4, num_layers=2):
        super().__init__()
        self.input_layer = nn.Linear(feature_dim, d_model)
        self.pos = PositionalEncoding(d_model)

        self.layers = nn.ModuleList([
            TransformerBlock(d_model, num_heads) for _ in range(num_layers)
        ])

        self.output_layer = nn.Linear(d_model, feature_dim)

    def forward(self, x):
        x = self.input_layer(x)
        x = self.pos(x)
        for layer in self.layers:
            x = layer(x)
        return self.output_layer(x[:, -1])

#8.TRAINING LOOP
def train(model, loader, epochs=5, lr=1e-3):
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    for epoch in range(epochs):
        for x, y in loader:
            opt.zero_grad()
            pred = model(x)
            loss = loss_fn(pred, y)
            loss.backward()
            opt.step()

        print(f"Epoch {epoch+1}, Loss = {loss.item():.4f}")
